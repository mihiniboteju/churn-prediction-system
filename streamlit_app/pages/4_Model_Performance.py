import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import numpy as np
import os

st.set_page_config(
    page_title="Model Performance",
    page_icon="üìä",
    layout="wide"
)

st.title("Model Performance & Evaluation")
st.markdown("### Deep dive into our machine learning model's predictive power")

st.markdown("---")

# Model Selection Summary
st.markdown("## Best Model: Logistic Regression")

col1, col2 = st.columns([2, 1])

with col1:
    st.markdown(""" After training and evaluating **3 different machine learning algorithms**, we selected **Logistic Regression** as our production model based on its superior performance in 
    identifying customers likely to churn (Recall score).
    
    ### Why Logistic Regression?
    
    - **Highest Recall** (53.21%): Best at catching actual churners
    - **Excellent ROC-AUC** (84.34%): Strong discriminative ability
    - **Interpretable**: Easy to explain predictions to business stakeholders
    - **Fast**: Real-time predictions with low computational cost
    - **Reliable**: Consistent performance on unseen data
    """)

with col2:
    st.markdown(""" <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                padding: 2rem; border-radius: 15px; color: white;'>
        <h3> Final Model Stats</h3>
        <hr style='border-color: rgba(255,255,255,0.3);'>
        <p><strong>Algorithm:</strong> Logistic Regression</p>
        <p><strong>Features:</strong> 37 (after VIF)</p>
        <p><strong>Training Samples:</strong> 5,634</p>
        <p><strong>Test Samples:</strong> 1,409</p>
        <p><strong>Training Time:</strong> < 1 second</p>
        <p><strong>Status:</strong>  Production Ready</p>
    </div>
    """, unsafe_allow_html=True)

# Performance Metrics
st.markdown("---")
st.markdown("## Performance Metrics Explained")

# Load actual metrics
@st.cache_data
def load_metrics():
    metrics_path = os.path.join("..", "data", "processed", "phase5c_model_metrics.csv")
    try:
        return pd.read_csv(metrics_path)
    except:
        # Fallback data
        return pd.DataFrame({
            'Model': ['Logistic Regression', 'Random Forest', 'Gradient Boosting'],
            'Accuracy': [0.8041, 0.7800, 0.7942],
            'Precision': [0.6633, 0.6067, 0.6364],
            'Recall': [0.5321, 0.4866, 0.5241],
            'F1-Score': [0.5905, 0.5401, 0.5748],
            'ROC-AUC': [0.8434, 0.8051, 0.8396]
        })

metrics_df = load_metrics()

# Key metrics display
metric_col1, metric_col2, metric_col3, metric_col4, metric_col5 = st.columns(5)

lr_metrics = metrics_df[metrics_df['Model'] == 'Logistic Regression'].iloc[0]

with metric_col1:
    st.metric(
        " Accuracy",
        f"{lr_metrics['Accuracy']*100:.2f}%",
        help="Overall correctness: (TP + TN) / Total" )

with metric_col2:
    st.metric(
        " Precision",
        f"{lr_metrics['Precision']*100:.2f}%",
        help="When we predict churn, we're right 66% of the time" )

with metric_col3:
    st.metric(
        " Recall",
        f"{lr_metrics['Recall']*100:.2f}%",
        help="We catch 53% of customers who actually churn" )

with metric_col4:
    st.metric(
        " F1-Score",
        f"{lr_metrics['F1-Score']*100:.2f}%",
        help="Harmonic mean of Precision and Recall" )

with metric_col5:
    st.metric(
        " ROC-AUC",
        f"{lr_metrics['ROC-AUC']*100:.2f}%",
        help="Excellent ability to distinguish churners" )

# Metrics explanation tabs
st.markdown("---")
st.markdown("### Understanding the Metrics")

tab1, tab2, tab3, tab4, tab5 = st.tabs([" Accuracy", " Precision", " Recall", " F1-Score", " ROC-AUC"])

with tab1:
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown(""" ### Accuracy: Overall Correctness **Definition:**Percentage of all predictions that were correct. **Formula:** `(True Positives + True Negatives) / Total Predictions` **Our Score:** 80.41% **What it means:** - Out of 1,409 customers, we correctly classified 1,133
        - 276 predictions were wrong **Interpretation:** - **Good**: Above 80% is solid performance
        - **Reliable**: 4 out of 5 predictions are correct
        - **Note**: Can be misleading with imbalanced data (26.5% churn rate)
        """)
    
    with col2:
        # Accuracy visualization
        fig = go.Figure(go.Indicator(
            mode="gauge+number",
            value=80.41,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': "Accuracy"},
            gauge={
                'axis': {'range': [None, 100]},
                'bar': {'color': "#2ECC71"},
                'steps': [
                    {'range': [0, 60], 'color': "#E74C3C"},
                    {'range': [60, 80], 'color': "#F39C12"},
                    {'range': [80, 100], 'color': "#D5F4E6"}
                ],
                'threshold': {
                    'line': {'color': "red", 'width': 4},
                    'thickness': 0.75,
                    'value': 90
                }
            }
        ))
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)

with tab2:
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown(""" ### Precision: Prediction Confidence **Definition:**When we predict "Churn", how often are we correct? **Formula:** `True Positives / (True Positives + False Positives)` **Our Score:** 66.33% **What it means:** - When model predicts a customer will churn, it's correct 66% of the time
        - 34% of "will churn" predictions are false alarms **Business Impact:** - Affects cost of retention campaigns
        - Higher precision = less wasted effort on happy customers
        - Trade-off: Higher precision often means lower recall **Our Case:** 66% is acceptable - we'd rather catch more churners (higher recall) 
        even if some predictions are false positives.
        """)
    
    with col2:
        # Precision visualization
        fig = go.Figure(go.Indicator(
            mode="gauge+number",
            value=66.33,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': "Precision"},
            gauge={
                'axis': {'range': [None, 100]},
                'bar': {'color': "#3498DB"},
                'steps': [
                    {'range': [0, 50], 'color': "#E74C3C"},
                    {'range': [50, 70], 'color': "#F39C12"},
                    {'range': [70, 100], 'color': "#D5F4E6"}
                ]
            }
        ))
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)

with tab3:
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown(""" ### Recall: Catching Actual Churners **Definition:**Of all customers who actually churned, how many did we catch? **Formula:** `True Positives / (True Positives + False Negatives)` **Our Score:** 53.21% **What it means:** - Out of 374 customers who churned, we identified 199
        - We missed 175 churners (False Negatives) **Business Impact:** - **MOST IMPORTANT METRIC** for churn prediction
        - Missing a churner means lost revenue
        - False Negatives are expensive! **Why 53% is acceptable:** - Baseline (random): ~27% (matching churn rate)
        - We're 2x better than random
        - Identifying 199 at-risk customers is valuable
        - Room for improvement in future iterations
        """)
    
    with col2:
        # Recall visualization with context
        fig = go.Figure()
        
        fig.add_trace(go.Bar(
            name='Caught',
            x=['Churners'],
            y=[199],
            marker_color='#2ECC71',
            text=['199<br>Caught'],
            textposition='inside'
        ))
        
        fig.add_trace(go.Bar(
            name='Missed',
            x=['Churners'],
            y=[175],
            marker_color='#E74C3C',
            text=['175<br>Missed'],
            textposition='inside'
        ))
        
        fig.update_layout(
            title="Recall: 199 out of 374 churners caught",
            barmode='stack',
            height=300,
            showlegend=True
        )
        
        st.plotly_chart(fig, use_container_width=True)

with tab4:
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown(""" ### F1-Score: Balanced Performance **Definition:**Harmonic mean of Precision and Recall. **Formula:** `2 √ó (Precision √ó Recall) / (Precision + Recall)` **Our Score:** 59.05% **What it means:** - Balances Precision (66%) and Recall (53%)
        - Single metric to compare models
        - Useful when both false positives and false negatives matter **Why use harmonic mean?** - Punishes extreme imbalances
        - If either Precision or Recall is low, F1 will be low
        - Better than arithmetic mean for classification **Our Interpretation:** - 59% F1 shows reasonable balance
        - Model doesn't over-optimize for one metric
        - Suitable for production deployment
        """)
    
    with col2:
        # F1-Score component visualization
        fig = go.Figure()
        
        fig.add_trace(go.Scatter(
            x=['Precision', 'F1-Score', 'Recall'],
            y=[66.33, 59.05, 53.21],
            mode='lines+markers+text',
            text=['66.33%', '59.05%', '53.21%'],
            textposition='top center',
            line=dict(color='#9B59B6', width=3),
            marker=dict(size=15, color='#9B59B6')
        ))
        
        fig.update_layout(
            title="F1-Score balances Precision & Recall",
            yaxis_title="Score (%)",
            height=300,
            showlegend=False
        )
        
        st.plotly_chart(fig, use_container_width=True)

with tab5:
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown(""" ### ROC-AUC: Discriminative Ability **Definition:**Area Under the Receiver Operating Characteristic Curve. **Our Score:** 84.34% **What it means:** - Measures model's ability to distinguish between churners and non-churners
        - Score of 84% is **EXCELLENT** - Interpretation scale:
          - 90-100%: Excellent
          - 80-90%: Very Good (‚Üê We're here!)
          - 70-80%: Good
          - 60-70%: Fair
          - 50%: No better than random **Why it's important:** - Threshold-independent: Works at any decision boundary
        - Robust to class imbalance
        - Shows overall model quality **Business Meaning:** - 84% chance that model ranks a random churner higher than a random non-churner
        - Strong confidence in model's predictions
        - Suitable for ranking customers by churn risk
        """)
    
    with col2:
        # ROC-AUC gauge
        fig = go.Figure(go.Indicator(
            mode="gauge+number",
            value=84.34,
            domain={'x': [0, 1], 'y': [0, 1]},
            title={'text': "ROC-AUC"},
            gauge={
                'axis': {'range': [50, 100]},
                'bar': {'color': "#E74C3C"},
                'steps': [
                    {'range': [50, 70], 'color': "#FFE5E5"},
                    {'range': [70, 80], 'color': "#FFF4CC"},
                    {'range': [80, 90], 'color': "#D5F4E6"},
                    {'range': [90, 100], 'color': "#A8E6CF"}
                ],
                'threshold': {
                    'line': {'color': "#2ECC71", 'width': 4},
                    'thickness': 0.75,
                    'value': 84.34
                }
            }
        ))
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True)

# Model Comparison
st.markdown("---")
st.markdown("## Model Comparison: Why Logistic Regression Won")

col1, col2 = st.columns(2)

with col1:
    # Metrics comparison bar chart
    fig = go.Figure()
    
    metrics_to_compare = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
    
    for model in metrics_df['Model']:
        model_data = metrics_df[metrics_df['Model'] == model].iloc[0]
        values = [model_data[metric] * 100 for metric in metrics_to_compare]
        
        fig.add_trace(go.Bar(
            name=model,
            x=metrics_to_compare,
            y=values,
            text=[f'{v:.1f}%' for v in values],
            textposition='outside'
        ))
    
    fig.update_layout(
        title="Model Performance Comparison",
        yaxis_title="Score (%)",
        barmode='group',
        height=400,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )
    
    st.plotly_chart(fig, use_container_width=True)

with col2:
    # Model comparison table
    st.markdown("### Detailed Comparison")
    
    display_df = metrics_df.copy()
    for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']:
        display_df[col] = display_df[col].apply(lambda x: f'{x*100:.2f}%')
    
    st.dataframe(
        display_df,
        use_container_width=True,
        hide_index=True
    )
    
    st.markdown(""" **Winner: Logistic Regression** - **Highest Recall** (53.21%)
    - **Highest ROC-AUC** (84.34%)
    - **Best Accuracy** (80.41%)
    - Fast & interpretable
    """)

# Confusion Matrix
st.markdown("---")
st.markdown("## Confusion Matrix: Detailed Breakdown")

st.markdown("""
The confusion matrix shows exactly where our model succeeds and fails. Each cell represents 
the count of predictions for each actual/predicted combination.
""")

# Actual confusion matrix values for Logistic Regression
# From Phase 5: TN=909, FP=126, FN=175, TP=199
tn, fp, fn, tp = 909, 126, 175, 199

col1, col2 = st.columns([2, 1])

with col1:
    # Create confusion matrix heatmap
    confusion_matrix = np.array([[tn, fp], [fn, tp]])
    
    fig = go.Figure(data=go.Heatmap(
        z=confusion_matrix,
        x=['Predicted: No Churn', 'Predicted: Churn'],
        y=['Actual: No Churn', 'Actual: Churn'],
        text=[[f'TN: {tn}<br> Correct', f'FP: {fp}<br> False Alarm'],
              [f'FN: {fn}<br> Missed', f'TP: {tp}<br> Caught']],
        texttemplate='%{text}',
        textfont={"size": 14},
        colorscale='RdYlGn',
        showscale=False
    ))
    
    fig.update_layout(
        title="Confusion Matrix - Logistic Regression",
        height=400,
        xaxis={'side': 'bottom'}
    )
    
    st.plotly_chart(fig, use_container_width=True)

with col2:
    st.markdown("### Breaking It Down")
    
    st.metric(
        " True Negatives (TN)",
        f"{tn:,}",
        help="Correctly predicted non-churners" )
    
    st.metric(
        " True Positives (TP)",
        f"{tp:,}",
        help="Correctly predicted churners" )
    
    st.metric(
        " False Positives (FP)",
        f"{fp:,}",
        help="Predicted churn, but stayed (False Alarm)" )
    
    st.metric(
        " False Negatives (FN)",
        f"{fn:,}",
        help="Predicted no churn, but left (Missed)" )
    
    total_correct = tn + tp
    total = tn + fp + fn + tp
    st.markdown(f""" --- **Total Correct:** {total_correct:,} / {total:,} **Accuracy:** {total_correct/total*100:.2f}%
    """)

# Business Impact Calculator
st.markdown("---")
st.markdown("## Business Impact Calculator")

st.markdown("""
See the real financial impact of the model by adjusting key business parameters.
""")

with st.expander(" **Calculate ROI and Business Value**"):
    calc_col1, calc_col2, calc_col3 = st.columns(3)
    
    with calc_col1:
        st.markdown("### Model Performance")
        st.metric("True Positives (Caught)", tp)
        st.metric("False Negatives (Missed)", fn)
        st.metric("False Positives (False Alarms)", fp)
    
    with calc_col2:
        st.markdown("### Business Parameters")
        
        clv = st.number_input(
            "Customer Lifetime Value ($)",
            min_value=500,
            max_value=5000,
            value=2500,
            step=100,
            help="Average revenue from a customer over their lifetime" )
        
        campaign_cost = st.number_input(
            "Retention Campaign Cost per Customer ($)",
            min_value=10,
            max_value=200,
            value=50,
            step=5,
            help="Cost to run retention campaign for one customer" )
        
        success_rate = st.slider(
            "Campaign Success Rate (%)",
            min_value=10,
            max_value=80,
            value=30,
            step=5,
            help="Percentage of targeted customers who are successfully retained" )
    
    with calc_col3:
        st.markdown("### Results")
        
        # Calculate business metrics
        customers_targeted = tp + fp  # All predicted churners
        campaign_total_cost = customers_targeted * campaign_cost
        
        # Revenue saved (only from true positives who are successfully retained)
        customers_retained = tp * (success_rate / 100)
        revenue_saved = customers_retained * clv
        
        # Revenue lost from missed churners
        revenue_lost = fn * clv
        
        # Wasted cost on false positives
        wasted_cost = fp * campaign_cost
        
        # Net benefit
        net_benefit = revenue_saved - campaign_total_cost
        roi = (net_benefit / campaign_total_cost * 100) if campaign_total_cost > 0 else 0
        
        st.metric(
            " Revenue Protected",
            f"${revenue_saved:,.0f}",
            help=f"{customers_retained:.0f} customers retained" )
        
        st.metric(
            " Campaign Cost",
            f"${campaign_total_cost:,.0f}",
            help=f"{customers_targeted} customers targeted" )
        
        st.metric(
            " Net Benefit",
            f"${net_benefit:,.0f}",
            delta=f"ROI: {roi:.1f}%" )
    
    st.markdown("---")
    st.markdown("### Additional Context")
    
    context_col1, context_col2 = st.columns(2)
    
    with context_col1:
        st.markdown(f""" **Revenue Analysis:** - Revenue Saved: ${revenue_saved:,.0f}
        - Revenue Lost (Missed): ${revenue_lost:,.0f}
        - Wasted on False Alarms: ${wasted_cost:,.0f}
        - Total Campaign Cost: ${campaign_total_cost:,.0f}
        """)
    
    with context_col2:
        st.markdown(f""" **Performance Summary:** - Customers Retained: {customers_retained:.0f}
        - Retention Success Rate: {success_rate}%
        - Net Business Value: ${net_benefit:,.0f}
        - ROI: {roi:.1f}%
        """)

# Next Steps
st.markdown("---")
st.info(""" **Ready to try it yourself?**
- **Try It Yourself**: Use the interactive prediction tool with real customer inputs
- See the model in action and understand what drives churn predictions!
""")

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    <p><strong>Model Status:</strong>  Production Ready | <strong>Last Updated:</strong> Phase 5 Completion</p>
</div>
""", unsafe_allow_html=True)
